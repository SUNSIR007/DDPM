#!/usr/bin/env python3
"""
MPSåŸºå‡†æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•Macä¸ŠMPSè®¾å¤‡çš„å¯ç”¨æ€§å’Œæ€§èƒ½ï¼Œå¹¶ä¸CPUè¿›è¡Œå¯¹æ¯”
"""

import torch
import time
import sys
from modules import UNet_conditional


def check_mps_availability():
    """æ£€æŸ¥MPSè®¾å¤‡çš„å¯ç”¨æ€§å’Œé…ç½®ä¿¡æ¯"""
    print("ğŸ” MPSè®¾å¤‡æ£€æŸ¥")
    print("=" * 50)
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥MPSæ˜¯å¦æ„å»º
    if torch.backends.mps.is_built():
        print("âœ… MPSå·²æ„å»ºåˆ°PyTorchä¸­")
    else:
        print("âŒ MPSæœªæ„å»ºåˆ°PyTorchä¸­")
        print("ğŸ’¡ è¯·å®‰è£…æ”¯æŒMPSçš„PyTorchç‰ˆæœ¬ (1.12.0+)")
        return False
    
    # æ£€æŸ¥MPSæ˜¯å¦å¯ç”¨
    if torch.backends.mps.is_available():
        print("âœ… MPSè®¾å¤‡å¯ç”¨")
        print("ğŸ æ£€æµ‹åˆ°Apple Siliconæˆ–Metalå…¼å®¹GPU")
        return True
    else:
        print("âŒ MPSè®¾å¤‡ä¸å¯ç”¨")
        print("ğŸ’¡ éœ€è¦macOS 12.3+å’ŒApple Silicon (M1/M2)æˆ–Metalå…¼å®¹GPU")
        return False


def benchmark_basic_operations():
    """åŸºå‡†æµ‹è¯•åŸºæœ¬å¼ é‡æ“ä½œ"""
    print("\nğŸ§ª åŸºæœ¬æ“ä½œæ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•é…ç½®
    sizes = [(1000, 1000), (2000, 2000), (3000, 3000)]
    operations = {
        "çŸ©é˜µä¹˜æ³•": lambda x: torch.matmul(x, x),
        "å…ƒç´ ç›¸åŠ ": lambda x: x + x,
        "ReLUæ¿€æ´»": lambda x: torch.relu(x),
        "å·ç§¯æ“ä½œ": lambda x: torch.nn.functional.conv2d(
            x.view(1, 1, x.shape[0], x.shape[1]), 
            torch.randn(1, 1, 3, 3, device=x.device), 
            padding=1
        )
    }
    
    devices = ["cpu"]
    if torch.backends.mps.is_available():
        devices.append("mps")
    
    results = {}
    
    for device_name in devices:
        print(f"\nğŸ“± æµ‹è¯•è®¾å¤‡: {device_name.upper()}")
        device = torch.device(device_name)
        results[device_name] = {}
        
        for size in sizes:
            print(f"  ğŸ“ å¼ é‡å¤§å°: {size[0]}x{size[1]}")
            results[device_name][size] = {}
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡
            x = torch.randn(size, device=device)
            
            for op_name, op_func in operations.items():
                try:
                    # é¢„çƒ­
                    for _ in range(3):
                        _ = op_func(x)
                    
                    # è®¡æ—¶
                    start_time = time.time()
                    for _ in range(10):
                        result = op_func(x)
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 10
                    results[device_name][size][op_name] = avg_time
                    print(f"    {op_name}: {avg_time:.4f}s")
                    
                except Exception as e:
                    print(f"    {op_name}: âŒ å¤±è´¥ ({str(e)[:50]}...)")
                    results[device_name][size][op_name] = None
    
    return results


def benchmark_unet_model():
    """åŸºå‡†æµ‹è¯•UNetæ¨¡å‹æ€§èƒ½"""
    print("\nğŸ—ï¸ UNetæ¨¡å‹æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•é…ç½®
    batch_sizes = [1, 2, 4]
    image_size = 64
    num_classes = 10
    
    devices = ["cpu"]
    if torch.backends.mps.is_available():
        devices.append("mps")
    
    results = {}
    
    for device_name in devices:
        print(f"\nğŸ“± æµ‹è¯•è®¾å¤‡: {device_name.upper()}")
        device = torch.device(device_name)
        results[device_name] = {}
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = UNet_conditional(num_classes=num_classes, device=device_name).to(device)
            model.eval()
            
            for batch_size in batch_sizes:
                print(f"  ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
                
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                x = torch.randn(batch_size, 3, image_size, image_size, device=device)
                t = torch.randint(1, 1000, (batch_size,), device=device)
                y = torch.randint(0, num_classes, (batch_size,), device=device)
                
                try:
                    # é¢„çƒ­
                    with torch.no_grad():
                        for _ in range(3):
                            _ = model(x, t, y)
                    
                    # è®¡æ—¶å‰å‘ä¼ æ’­
                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(5):
                            output = model(x, t, y)
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 5
                    results[device_name][batch_size] = avg_time
                    
                    print(f"    å‰å‘ä¼ æ’­: {avg_time:.4f}s")
                    print(f"    è¾“å‡ºå½¢çŠ¶: {output.shape}")
                    print(f"    å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() if device_name == 'cuda' else 'N/A'}")
                    
                except Exception as e:
                    print(f"    âŒ æ‰¹æ¬¡å¤§å° {batch_size} å¤±è´¥: {str(e)[:100]}...")
                    results[device_name][batch_size] = None
                    
        except Exception as e:
            print(f"  âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)[:100]}...")
            results[device_name] = None
    
    return results


def print_performance_summary(basic_results, unet_results):
    """æ‰“å°æ€§èƒ½æ€»ç»“"""
    print("\nğŸ“Š æ€§èƒ½æ€»ç»“")
    print("=" * 50)
    
    if "mps" in basic_results and "cpu" in basic_results:
        print("ğŸš€ MPS vs CPU åŸºæœ¬æ“ä½œåŠ é€Ÿæ¯”:")
        
        for size in basic_results["cpu"]:
            print(f"\n  ğŸ“ å¼ é‡å¤§å° {size[0]}x{size[1]}:")
            for op_name in basic_results["cpu"][size]:
                cpu_time = basic_results["cpu"][size][op_name]
                mps_time = basic_results["mps"][size][op_name]
                
                if cpu_time and mps_time and cpu_time > 0:
                    speedup = cpu_time / mps_time
                    if speedup > 1:
                        print(f"    {op_name}: {speedup:.2f}x æ›´å¿« ğŸš€")
                    else:
                        print(f"    {op_name}: {1/speedup:.2f}x æ›´æ…¢ ğŸŒ")
                else:
                    print(f"    {op_name}: æ— æ³•æ¯”è¾ƒ")
    
    if "mps" in unet_results and "cpu" in unet_results:
        print("\nğŸ—ï¸ MPS vs CPU UNetæ¨¡å‹åŠ é€Ÿæ¯”:")
        
        for batch_size in unet_results["cpu"]:
            cpu_time = unet_results["cpu"][batch_size]
            mps_time = unet_results["mps"][batch_size]
            
            if cpu_time and mps_time and cpu_time > 0:
                speedup = cpu_time / mps_time
                if speedup > 1:
                    print(f"  æ‰¹æ¬¡å¤§å° {batch_size}: {speedup:.2f}x æ›´å¿« ğŸš€")
                else:
                    print(f"  æ‰¹æ¬¡å¤§å° {batch_size}: {1/speedup:.2f}x æ›´æ…¢ ğŸŒ")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ Mac MPSæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥MPSå¯ç”¨æ€§
    if not check_mps_availability():
        print("\nâŒ MPSä¸å¯ç”¨ï¼Œä»…è¿›è¡ŒCPUæµ‹è¯•")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    basic_results = benchmark_basic_operations()
    unet_results = benchmark_unet_model()
    
    # æ‰“å°æ€»ç»“
    print_performance_summary(basic_results, unet_results)
    
    print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ!")
    print("ğŸ’¡ å¦‚æœMPSæ˜¾ç¤ºè‰¯å¥½çš„åŠ é€Ÿæ•ˆæœï¼Œæ‚¨å¯ä»¥åœ¨è®­ç»ƒä¸­ä½¿ç”¨MPSè®¾å¤‡")


if __name__ == "__main__":
    main()
