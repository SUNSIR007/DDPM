#!/usr/bin/env python3
"""
ä»ä¸­æ–­ç‚¹æ¢å¤DDPMè®­ç»ƒ
Resume DDPM training from interrupted checkpoint
"""

import os
import sys
import torch
import argparse
from datetime import datetime

# å¯¼å…¥ç°æœ‰æ¨¡å—
from ddpm_conditional import Diffusion, train, get_optimal_device, optimize_batch_size_for_device
from modules import UNet_conditional, EMA
from utils import setup_logging, get_data
from mps_training_monitor import get_monitor
from early_stopping_monitor import EarlyStoppingMonitor

import torch.nn as nn
from torch import optim
from torch.utils.tensorboard import SummaryWriter
import copy
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(format="%(asctime)s - %(levelname)s: %(message)s", level=logging.INFO, datefmt="%I:%M:%S")


def load_interrupted_checkpoint(model_dir, run_name, device):
    """
    åŠ è½½ä¸­æ–­çš„æ£€æŸ¥ç‚¹
    """
    interrupted_files = {
        'model': os.path.join(model_dir, 'interrupted_ckpt.pt'),
        'ema_model': os.path.join(model_dir, 'interrupted_ema_ckpt.pt'), 
        'optimizer': os.path.join(model_dir, 'interrupted_optim.pt')
    }
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing_files = []
    for name, path in interrupted_files.items():
        if not os.path.exists(path):
            missing_files.append(f"{name}: {path}")
    
    if missing_files:
        print(f"âŒ ç¼ºå°‘ä¸­æ–­æ£€æŸ¥ç‚¹æ–‡ä»¶:")
        for missing in missing_files:
            print(f"   - {missing}")
        return None
    
    print(f"ğŸ“ æ‰¾åˆ°ä¸­æ–­æ£€æŸ¥ç‚¹æ–‡ä»¶:")
    for name, path in interrupted_files.items():
        file_size = os.path.getsize(path) / (1024*1024)  # MB
        mod_time = datetime.fromtimestamp(os.path.getmtime(path))
        print(f"   âœ… {name}: {file_size:.1f}MB (ä¿®æ”¹æ—¶é—´: {mod_time.strftime('%Y-%m-%d %H:%M:%S')})")
    
    try:
        # åŠ è½½æ£€æŸ¥ç‚¹
        model_checkpoint = torch.load(interrupted_files['model'], map_location=device)
        ema_checkpoint = torch.load(interrupted_files['ema_model'], map_location=device)  
        optim_checkpoint = torch.load(interrupted_files['optimizer'], map_location=device)
        
        print("âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        return {
            'model_state_dict': model_checkpoint,
            'ema_model_state_dict': ema_checkpoint,
            'optimizer_state_dict': optim_checkpoint
        }
    except Exception as e:
        print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹æ—¶å‡ºé”™: {e}")
        return None


def resume_train(args):
    """
    æ¢å¤è®­ç»ƒä¸»å‡½æ•°
    """
    print("ğŸ”„ ä»ä¸­æ–­ç‚¹æ¢å¤DDPMè®­ç»ƒ")
    print("=" * 60)
    
    # ğŸ” åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
    monitor = get_monitor(args.device)
    monitor.start_monitoring()
    monitor.print_system_info()
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    device = torch.device(args.device)
    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè®­ç»ƒç›®å½•
    setup_logging(args.run_name)
    
    # åŠ è½½ä¸­æ–­çš„æ£€æŸ¥ç‚¹
    model_dir = os.path.join("models", args.run_name)
    checkpoints = load_interrupted_checkpoint(model_dir, args.run_name, device)
    
    if checkpoints is None:
        print("âŒ æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹ï¼Œé€€å‡ºç¨‹åº")
        return False
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹...")
    model = UNet_conditional(num_classes=args.num_classes, device=args.device, img_size=args.image_size).to(device)
    ema_model = copy.deepcopy(model).eval().requires_grad_(False)
    optimizer = optim.AdamW(model.parameters(), lr=args.lr)
    
    # åŠ è½½æ£€æŸ¥ç‚¹çŠ¶æ€
    print("ğŸ“‚ æ¢å¤æ¨¡å‹çŠ¶æ€...")
    model.load_state_dict(checkpoints['model_state_dict'])
    ema_model.load_state_dict(checkpoints['ema_model_state_dict'])
    optimizer.load_state_dict(checkpoints['optimizer_state_dict'])
    
    # åˆ›å»ºå…¶ä»–ç»„ä»¶
    dataloader = get_data(args)
    mse = nn.MSELoss()
    diffusion = Diffusion(img_size=args.image_size, device=device)
    logger = SummaryWriter(os.path.join("runs", args.run_name))
    l = len(dataloader)
    ema = EMA(0.995)
    
    # ğŸ›¡ï¸ åˆå§‹åŒ–æ—©åœç›‘æ§å™¨
    early_stopping = EarlyStoppingMonitor(
        patience=25,
        min_delta=1e-6,
        smoothing_window=10,
        convergence_threshold=1e-5,
        auto_stop=False,
        save_plots=True
    )
    
    print("âœ… æ¨¡å‹æ¢å¤å®Œæˆï¼Œå¼€å§‹ç»§ç»­è®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒå‚æ•°: epochs={args.epochs}, batch_size={args.batch_size}, lr={args.lr}")
    print("=" * 60)
    
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    epoch_losses = []
    start_epoch = args.resume_epoch if hasattr(args, 'resume_epoch') else 0
    
    try:
        for epoch in range(start_epoch, args.epochs):
            # ğŸ” è®°å½•epochå¼€å§‹
            monitor.log_epoch_start(epoch, args.epochs)
            logging.info(f"æ¢å¤è®­ç»ƒ Epoch {epoch}/{args.epochs}:")
            
            # åˆ›å»ºè¿›åº¦æ¡
            from tqdm import tqdm
            pbar = tqdm(dataloader, desc=f"Epoch {epoch}")
            
            epoch_loss_sum = 0.0
            batch_count = 0
            
            # è®­ç»ƒå¾ªç¯
            for i, (images, labels) in enumerate(pbar):
                images = images.to(device)
                labels = labels.to(device)
                t = diffusion.sample_timesteps(images.shape[0]).to(device)
                x_t, noise = diffusion.noise_images(images, t)
                
                # Classifier-Free Guidanceè®­ç»ƒç­–ç•¥
                import numpy as np
                if np.random.random() < 0.1:
                    labels = None
                    
                predicted_noise = model(x_t, t, labels)
                loss = mse(noise, predicted_noise)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                ema.step_ema(ema_model, model)
                
                # è®°å½•æŸå¤±
                loss_value = loss.item()
                epoch_loss_sum += loss_value
                batch_count += 1
                
                # ğŸ” è®°å½•æ‰¹æ¬¡è¿›åº¦
                monitor.log_batch_progress(i, len(dataloader), loss_value, args.lr)
                
                pbar.set_postfix(MSE=loss_value, Device=args.device)
                logger.add_scalar("MSE", loss_value, global_step=epoch * l + i)
            
            # è®¡ç®—epochå¹³å‡æŸå¤±
            avg_epoch_loss = epoch_loss_sum / batch_count if batch_count > 0 else 0.0
            epoch_losses.append(avg_epoch_loss)
            
            # ğŸ” è®°å½•epochç»“æŸ
            monitor.log_epoch_end(epoch, avg_epoch_loss)
            
            # ğŸ›¡ï¸ æ›´æ–°æ—©åœç›‘æ§å™¨
            should_stop, stop_reason = early_stopping.update(avg_epoch_loss, epoch)
            
            # æ¯5ä¸ªepochæ‰“å°ç›‘æ§çŠ¶æ€
            if epoch % 5 == 0:
                print("\n" + early_stopping.get_status_summary(epoch))

            # æ‰‹åŠ¨åœæ­¢æ£€æŸ¥
            manual_stop, manual_reason = early_stopping.manual_stop_check()

            stop_triggered = should_stop or manual_stop
            final_stop_reason = manual_reason if manual_stop else stop_reason

            # å½“è¾¾åˆ°é‡‡æ ·é—´éš”æˆ–æ£€æµ‹åˆ°åœæ­¢æ—¶æ‰§è¡Œé‡‡æ ·ä¸æ£€æŸ¥ç‚¹ä¿å­˜
            if stop_triggered or epoch % 10 == 0:
                labels = torch.arange(10).long().to(device)
                monitor.log_sampling_start(len(labels))

                sampled_images = diffusion.sample(model, n=len(labels), labels=labels)
                ema_sampled_images = diffusion.sample(ema_model, n=len(labels), labels=labels)

                from utils import save_images
                main_image_path = os.path.join("results", args.run_name, f"{epoch}.jpg")
                ema_image_path = os.path.join("results", args.run_name, f"{epoch}_ema.jpg")

                save_images(sampled_images, main_image_path, nrow=len(labels))
                save_images(ema_sampled_images, ema_image_path, nrow=len(labels))

                monitor.log_sampling_end(len(labels), f"{main_image_path}, {ema_image_path}")

                torch.save(model.state_dict(), os.path.join("models", args.run_name, f"ckpt.pt"))
                torch.save(ema_model.state_dict(), os.path.join("models", args.run_name, f"ema_ckpt.pt"))
                torch.save(optimizer.state_dict(), os.path.join("models", args.run_name, f"optim.pt"))

                print(f"âœ… Epoch {epoch}: æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜")

            if stop_triggered:
                reason_to_report = final_stop_reason or "æ—©åœæ¡ä»¶è§¦å‘"
                print(f"\nğŸ›‘ è®­ç»ƒåœæ­¢æ¡ä»¶å·²è§¦å‘: {reason_to_report}")
                early_stopping.save_checkpoint_with_metadata(
                    model, ema_model, optimizer, epoch, avg_epoch_loss,
                    os.path.join("models", args.run_name), args.run_name
                )
                early_stopping.save_loss_plot(
                    os.path.join("results", args.run_name), args.run_name
                )
                break
    
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        monitor.print_training_summary(args.epochs)
        print("ğŸ’¾ æ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
        
        # ä¿å­˜ä¸­æ–­æ—¶çš„çŠ¶æ€
        torch.save(model.state_dict(), os.path.join("models", args.run_name, f"interrupted_ckpt.pt"))
        torch.save(ema_model.state_dict(), os.path.join("models", args.run_name, f"interrupted_ema_ckpt.pt"))
        torch.save(optimizer.state_dict(), os.path.join("models", args.run_name, f"interrupted_optim.pt"))
        
        # ä¿å­˜æ—©åœç›‘æ§å™¨æ•°æ®
        try:
            early_stopping.save_loss_plot(
                os.path.join("results", args.run_name), f"{args.run_name}_interrupted"
            )
            print("ğŸ“ˆ è®­ç»ƒåˆ†æå›¾è¡¨å·²ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åˆ†æå›¾è¡¨æ—¶å‡ºé”™: {e}")
        
        print("âœ… ä¸­æ–­çŠ¶æ€å·²ä¿å­˜")
    
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        monitor.print_training_summary(args.epochs)
        raise
    
    finally:
        # æ‰“å°æœ€ç»ˆæ€»ç»“
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        monitor.print_training_summary(args.epochs)
        
        # æ—©åœç›‘æ§å™¨æ€»ç»“
        if 'early_stopping' in locals():
            print("\nğŸ›¡ï¸ æ—©åœç›‘æ§å™¨æ€»ç»“:")
            if len(early_stopping.loss_history) > 0:
                print(f"   ğŸ“‰ æ€»å…±è®°å½•äº† {len(early_stopping.loss_history)} ä¸ªepochçš„loss")
                print(f"   ğŸ† æœ€ä½³loss: {early_stopping.best_loss:.6f} (Epoch {early_stopping.best_epoch})")
                print(f"   ğŸ¯ æ”¶æ•›çŠ¶æ€: {'å·²æ”¶æ•›' if early_stopping.is_converged else 'æœªæ”¶æ•›'}")
            
            # ä¿å­˜æœ€ç»ˆåˆ†æå›¾
            try:
                early_stopping.save_loss_plot(
                    os.path.join("results", args.run_name), f"{args.run_name}_resumed"
                )
                print("   ğŸ“ˆ æœ€ç»ˆåˆ†æå›¾è¡¨å·²ä¿å­˜")
            except Exception as e:
                print(f"   âš ï¸ ä¿å­˜æœ€ç»ˆåˆ†æå›¾è¡¨æ—¶å‡ºé”™: {e}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Resume DDPM training from interrupted checkpoint")
    parser.add_argument('--epochs', type=int, default=300, help='Total epochs to train')
    parser.add_argument('--batch_size', type=int, default=None, help='Batch size (auto if not specified)')
    parser.add_argument('--lr', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--resume_epoch', type=int, default=0, help='Epoch to resume from')
    
    args = parser.parse_args()
    
    print("ğŸ”„ DDPMè®­ç»ƒæ¢å¤ç¨‹åº")
    print("=" * 50)
    
    # è®¾ç½®åŸºæœ¬å‚æ•°
    args.run_name = "DDPM_conditional"
    args.image_size = 64
    args.num_classes = 10
    args.train = True
    
    # æ•°æ®é›†è·¯å¾„é…ç½®
    possible_paths = [
        "./datasets/cifar10-64/train",
        "./data/cifar10-64/train",
        "../datasets/cifar10-64/train",
        "/home/ryuichi/datasets/cifar10-64/train",
        "/tmp/cifar10-64/train"
    ]
    
    dataset_found = False
    for path in possible_paths:
        if os.path.exists(path):
            args.dataset_path = path
            dataset_found = True
            print(f"ğŸ“Š æ‰¾åˆ°æ•°æ®é›†: {path}")
            break
    
    if not dataset_found:
        print("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
        args.dataset_path = possible_paths[0]
    
    # è·å–æœ€ä¼˜è®¾å¤‡
    device_name, _, device_info = get_optimal_device()
    args.device = device_name
    
    # ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
    if args.batch_size is None:
        base_batch_size = 4
        args.batch_size = optimize_batch_size_for_device(device_name, base_batch_size)
    
    # CUDAä¼˜åŒ–è®¾ç½®
    if device_name == "cuda":
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print("ğŸ”§ å·²å¯ç”¨CUDAä¼˜åŒ–")
    
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"   ğŸ¯ è¿è¡Œåç§°: {args.run_name}")
    print(f"   ğŸ”„ æ€»è½®æ•°: {args.epochs}")
    print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"   ğŸ–¼ï¸ å›¾åƒå°ºå¯¸: {args.image_size}x{args.image_size}")
    print(f"   ğŸ·ï¸ ç±»åˆ«æ•°: {args.num_classes}")
    print(f"   ğŸ“š æ•°æ®é›†: {args.dataset_path}")
    print(f"   ğŸ›ï¸ è®¾å¤‡: {device_info}")
    print(f"   ğŸ“ˆ å­¦ä¹ ç‡: {args.lr}")
    print("=" * 50)
    
    # å¼€å§‹æ¢å¤è®­ç»ƒ
    success = resume_train(args)
    
    if success:
        print("\nâœ… è®­ç»ƒæ¢å¤å®Œæˆ!")
    else:
        print("\nâŒ è®­ç»ƒæ¢å¤å¤±è´¥!")
        sys.exit(1)


if __name__ == '__main__':
    main()
